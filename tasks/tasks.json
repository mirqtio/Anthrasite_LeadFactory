{
  "tasks": [
    {
      "id": 1,
      "title": "Create CI Infrastructure and Gating System",
      "description": "Establish a CI pipeline with GitHub Actions that includes linting, testing, and security scanning gates.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "1. Create a GitHub Actions workflow file at `.github/workflows/ci.yml`\n2. Configure the workflow to run on push to main and pull requests\n3. Set up Python 3.10.x environment in the workflow\n4. Add steps for installing dependencies from requirements files\n5. Configure linting steps (black, ruff) in warning mode initially\n6. Set up test execution with pytest\n7. Add security scanning with Bandit in audit mode\n8. Configure test coverage reporting\n9. Set up artifact collection for logs and reports\n10. Add status badges to README.md\n\nExample workflow file structure:\n```yaml\nname: CI\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; else pip install -r requirements.txt; fi\n      - name: Lint with Black (warning mode)\n        run: |\n          black --check . || echo \"Black formatting issues found (warning only)\"\n      - name: Lint with Ruff (warning mode)\n        run: |\n          ruff check . || echo \"Ruff linting issues found (warning only)\"\n      - name: Security scan with Bandit\n        run: |\n          bandit -r . -o bandit-report.txt -f txt || echo \"Security issues found (audit only)\"\n      - name: Test with pytest\n        run: |\n          pytest --cov=leadfactory\n      - name: Upload reports\n        uses: actions/upload-artifact@v3\n        with:\n          name: reports\n          path: |\n            bandit-report.txt\n            .coverage\n```",
      "testStrategy": "1. Verify that the GitHub Actions workflow file is correctly structured with `yamllint .github/workflows/ci.yml`\n2. Test the workflow locally using `act` if available\n3. Make a test commit to trigger the workflow and verify it runs correctly\n4. Check that all specified steps are included in the workflow\n5. Verify that the workflow correctly identifies linting issues but doesn't fail the build (warning mode)\n6. Confirm that test execution works and generates coverage reports\n7. Validate that security scanning runs and produces the expected output\n8. Check that artifacts are correctly uploaded and accessible\n9. Verify that status badges are correctly added to README.md and display the current status\n10. Run a post-execution healthcheck that confirms the CI workflow exists and is valid",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Environment & Tooling Alignment",
      "description": "Pin Python version to 3.10.x across all environments and create properly separated requirements files for development and production dependencies.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Create or update `.python-version` file with '3.10.x'\n2. Update CI configuration in GitHub Actions to use `actions/setup-python@v4` with Python 3.10.x\n3. Update Dockerfile base image to use Python 3.10.x if present\n4. Create `requirements.txt` with production dependencies (fastapi, requests, etc.) with exact version pins\n5. Create `requirements-dev.txt` with development dependencies (black, ruff, bandit, mypy, pytest) with exact version pins\n6. Add a version logging step to CI that prints all tool versions on every run\n7. Create or update `.env.example` with all environment variables used in the application\n8. Create a centralized config module at `leadfactory/config.py` that loads all environment variables consistently\n\nExample config module:\n```python\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# API Keys\nYELP_API_KEY = os.getenv('YELP_API_KEY')\nGOOGLE_PLACES_API_KEY = os.getenv('GOOGLE_PLACES_API_KEY')\nSCREENSHOTONE_API_KEY = os.getenv('SCREENSHOTONE_API_KEY')\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nSENDGRID_API_KEY = os.getenv('SENDGRID_API_KEY')\n\n# Budget and Cost Settings\nMAX_COST_PER_LEAD = float(os.getenv('MAX_COST_PER_LEAD', '0.5'))\nBUDGET_ALERT_THRESHOLD = float(os.getenv('BUDGET_ALERT_THRESHOLD', '0.8'))\n\n# Other settings\n# ...\n```",
      "testStrategy": "1. Verify that Python 3.10.x is correctly specified in all configuration files with `grep -r \"3.10\" .`\n2. Confirm that CI workflow logs the correct Python and tool versions by checking CI logs\n3. Test that all dependencies can be installed from requirements files without conflicts using `pip install -r requirements.txt` and `pip install -r requirements-dev.txt`\n4. Validate that the config module correctly loads all environment variables from .env file with a dedicated test\n5. Create a test that verifies all environment variables in the codebase are documented in .env.example\n6. Verify CI passes with all linting and test steps\n7. Check for expected output files: `.python-version`, `requirements.txt`, `requirements-dev.txt`, `.env.example`, and `leadfactory/config.py`\n8. Run a post-execution healthcheck script that validates all configuration is correctly applied",
      "subtasks": [
        {
          "id": 1,
          "title": "Python Version Standardization",
          "description": "Pin Python version to 3.10.x across all environments by updating configuration files",
          "dependencies": [],
          "details": "1. Create or update `.python-version` file with '3.10.x'\n2. Update CI configuration in GitHub Actions to use `actions/setup-python@v4` with Python 3.10.x\n3. Update Dockerfile base image to use Python 3.10.x if present",
          "status": "done",
          "testStrategy": "Verify that all environments correctly use Python 3.10.x by adding version check commands to CI pipeline and running `python --version` to confirm the correct version is active"
        },
        {
          "id": 2,
          "title": "Requirements File Separation",
          "description": "Create separate requirements files for production and development dependencies with exact version pins",
          "dependencies": [
            1
          ],
          "details": "1. Create `requirements.txt` with production dependencies (fastapi, requests, etc.) with exact version pins\n2. Create `requirements-dev.txt` with development dependencies (black, ruff, bandit, mypy, pytest) with exact version pins\n3. Ensure `-r requirements.txt` is included in requirements-dev.txt to inherit production dependencies",
          "status": "done",
          "testStrategy": "Verify installation works in both production and development environments by testing `pip install -r requirements.txt` and `pip install -r requirements-dev.txt` commands in CI and checking for successful completion"
        },
        {
          "id": 3,
          "title": "CI Pipeline Enhancement",
          "description": "Add version logging to CI pipeline to ensure consistent tooling across builds",
          "dependencies": [
            1,
            2
          ],
          "details": "1. Add a version logging step to CI that prints Python version\n2. Add commands to print versions of all installed packages\n3. Add commands to print versions of key development tools (black, ruff, etc.)",
          "status": "done",
          "testStrategy": "Review CI logs to confirm all version information is properly displayed and add a post-execution check that validates the expected tool versions are present in the logs"
        },
        {
          "id": 4,
          "title": "Environment Variables Documentation",
          "description": "Create a comprehensive example of all required environment variables",
          "dependencies": [],
          "details": "1. Create or update `.env.example` with all environment variables used in the application\n2. Include comments explaining the purpose of each variable\n3. Group variables by category (API keys, settings, etc.)",
          "status": "done",
          "testStrategy": "Verify that all environment variables referenced in the codebase are included in the example file with `grep -r \"os.getenv\" --include=\"*.py\" . | sort | uniq` and compare with `.env.example`"
        },
        {
          "id": 5,
          "title": "Centralized Configuration Module",
          "description": "Create a centralized config module that consistently loads all environment variables",
          "dependencies": [
            4
          ],
          "details": "1. Create `leadfactory/config.py` module\n2. Implement dotenv loading\n3. Add typed variables for all environment variables with appropriate defaults\n4. Group configuration by logical categories (API keys, budget settings, etc.)\n5. Add docstrings explaining the purpose of each configuration variable",
          "status": "done",
          "testStrategy": "Write unit tests to verify config module correctly loads variables from environment and applies proper defaults, and run a post-execution check that validates the config module exists and contains all required variables"
        }
      ]
    },
    {
      "id": 3,
      "title": "Establish Linting, Formatting, and Type Checking Baseline",
      "description": "Apply code formatting with Black, configure Ruff for minimal linting, set up MyPy for type checking, and run Bandit for security scanning in audit mode.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Run `black .` on the entire codebase to establish consistent formatting\n2. Configure Ruff with a `.ruff.toml` file to enforce:\n   - F401 (unused imports)\n   - F841 (unused variables)\n   - B002 (known bugs)\n   - Ignore E501 (line length), E722 (bare except), etc. for now\n3. Set up MyPy in non-strict mode for `utils/`, `bin/`, and `tests/` directories\n4. Create a MyPy configuration file (`mypy.ini` or `pyproject.toml` section)\n5. Run Bandit in audit mode and save the output to a report file\n6. Create a `lint_debt.md` file documenting any remaining `# type: ignore` comments or lint suppressions\n7. Add CI steps for running these tools\n\nExample `.ruff.toml`:\n```toml\nselect = [\"F401\", \"F841\", \"B002\"]\nignore = [\"E501\", \"E722\"]\ntarget-version = \"py310\"\nline-length = 100\n```\n\nExample `mypy.ini`:\n```ini\n[mypy]\npython_version = 3.10\ndisallow_untyped_defs = False\ndisallow_incomplete_defs = False\ncheck_untyped_defs = True\ndisallow_untyped_decorators = False\nno_implicit_optional = True\nstrict_optional = True\n\n[mypy.plugins.numpy.*]\nfollow_imports = skip\n\n[mypy-utils.*]\ncheck_untyped_defs = True\n\n[mypy-bin.*]\ncheck_untyped_defs = True\n\n[mypy-tests.*]\ncheck_untyped_defs = True\n```",
      "testStrategy": "1. Verify that Black formatting is applied consistently by running `black --check .` in CI\n2. Confirm that Ruff linting passes with the specified rules by running `ruff check .` in CI\n3. Validate that MyPy type checking passes on the specified directories with `mypy utils/ bin/ tests/` in CI\n4. Ensure Bandit security scanning runs without critical issues using `bandit -r . -o bandit-report.json` in CI\n5. Verify that CI workflow includes all these checks and reports results correctly\n6. Check that `lint_debt.md` is created and contains documentation of all suppressions\n7. Run a post-execution healthcheck that verifies all configuration files exist: `.ruff.toml`, `mypy.ini` or appropriate `pyproject.toml` section\n8. Confirm CI passes with all linting steps enabled\n9. Verify expected output files exist: `lint_debt.md` and `bandit-report.json`",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Clean Up Dead Code, Clutter, and Legacy Artifacts",
      "description": "Remove redundant tests, patch scripts, backup files, commented-out code, print statements, and resolve TODOs/FIXMEs in the codebase.",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "medium",
      "details": "1. Identify and delete redundant test files:\n   - `test_dedupe_new.py`\n   - `test_mockup 3.py`\n   - `test_unsubscribe 3.py`\n2. Remove patch or fix scripts:\n   - `fix_test_mockup_unit.py`\n   - `fix_dedupe_simple.py`\n3. Delete backup files with extensions like `.bak`\n4. Search for and remove all commented-out code blocks\n5. Remove all `print()` statements in production logic (not tests)\n6. Delete unused test files and legacy scripts in `tmp/` directories\n7. Search for TODO/FIXME comments and either implement the required changes or document them in `lint_debt.md`\n8. Use regular expressions to find patterns of dead code:\n   ```\n   # Search patterns:\n   # - \"^\\s*#(?!\\s*[a-zA-Z]+:)\" (commented lines not part of docstrings)\n   # - \"print\\(\" (print statements)\n   # - \"TODO|FIXME|XXX|HACK\" (code markers)\n   # - \"if\\s+False:\" (unreachable code)\n   ```\n9. Maintain a log of all removed files and major code blocks for reference",
      "testStrategy": "1. Run the test suite after each major removal to ensure functionality is preserved with `pytest`\n2. Verify that CI still passes after cleanup with all linting and test steps\n3. Check that no new lint warnings are introduced with `ruff check .`\n4. Create a test that scans for print statements in production code using `grep -r \"print(\" --include=\"*.py\" . | grep -v \"tests/\"`\n5. Validate that all removed test files are either redundant or consolidated elsewhere\n6. Ensure no functionality is lost by comparing test coverage before and after cleanup\n7. Run a post-execution healthcheck that confirms the absence of the specified redundant files\n8. Verify the absence of `.bak` files with `find . -name \"*.bak\" | wc -l`\n9. Check that a cleanup log file exists documenting all removed artifacts",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Consolidate and Enhance Test Suite",
      "description": "Consolidate duplicate tests, ensure comprehensive test coverage for all pipeline stages, and fix or remove skipped tests.",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "1. Consolidate all `test_dedupe*.py` files into a single comprehensive test module\n2. Organize duplicate helper tests under `tests/utils/` into logical groupings\n3. Audit all pipeline stages to ensure each has:\n   - At least one unit test\n   - At least one BDD test (using pytest-bdd)\n   - At least one integration test with mock/fake API calls\n4. Review all tests marked with `@pytest.mark.skip` and either:\n   - Implement the missing test logic\n   - Remove the test if redundant or no longer applicable\n5. Complete any partial BDD scenarios (particularly for scoring functionality)\n6. Replace any `pass` statements in tests with actual assertions\n7. Add test fixtures for common test setup and teardown\n8. Implement parameterized tests where appropriate to increase coverage\n\nExample consolidated test structure:\n```\ntests/\n  conftest.py  # shared fixtures\n  unit/\n    test_dedupe.py  # consolidated from multiple files\n    test_scoring.py\n    utils/\n      test_io.py  # consolidated helpers\n  integration/\n    test_pipeline_stages.py  # with mock APIs\n  bdd/\n    features/\n      pipeline.feature\n    step_defs/\n      test_pipeline_steps.py\n```",
      "testStrategy": "1. Measure test coverage before and after consolidation to ensure no reduction using `pytest --cov=leadfactory`\n2. Verify that all pipeline stages have the required test types with a test inventory check\n3. Confirm that no tests are skipped in the final test suite by running `pytest -v | grep -c \"SKIPPED\"`\n4. Run the full test suite to ensure all tests pass with `pytest`\n5. Check that BDD scenarios are complete and properly implemented with `pytest tests/bdd`\n6. Validate that CI runs all tests and reports results correctly\n7. Use pytest's `--durations=10` flag to identify slow tests for potential optimization\n8. Run a post-execution healthcheck that verifies the expected test structure exists\n9. Check for the absence of `pass` statements in tests with `grep -r \"pass\" --include=\"test_*.py\" .`\n10. Verify that CI passes with the consolidated test suite\n11. Confirm the existence of expected test files and directories",
      "subtasks": [
        {
          "id": 1,
          "title": "Consolidate Duplicate Test Files",
          "description": "Merge all test_dedupe*.py files into a single comprehensive module and organize duplicate helper tests into logical groupings.",
          "dependencies": [],
          "details": "1. Identify all test_dedupe*.py files in the codebase\n2. Create a new test_dedupe.py file in tests/unit/\n3. Merge test cases from all identified files, removing duplicates\n4. Organize helper tests under tests/utils/ into logical groups\n5. Update imports in all affected test files",
          "status": "done",
          "testStrategy": "Manual verification that all test cases from original files are present in consolidated files and all tests pass"
        },
        {
          "id": 2,
          "title": "Implement Test Coverage for Pipeline Stages",
          "description": "Audit all pipeline stages and ensure each has unit, BDD, and integration tests with mock API calls.",
          "dependencies": [],
          "details": "1. Identify all pipeline stages in the codebase\n2. Create a coverage matrix tracking unit/BDD/integration test status for each stage\n3. Implement missing unit tests for each stage\n4. Create BDD scenarios using pytest-bdd for each stage\n5. Develop integration tests with mock/fake API calls for each stage",
          "status": "done",
          "testStrategy": "Use pytest-cov to verify test coverage meets minimum thresholds for each pipeline stage"
        },
        {
          "id": 3,
          "title": "Fix or Remove Skipped Tests",
          "description": "Review all tests marked with @pytest.mark.skip and either implement the missing logic or remove if redundant.",
          "dependencies": [],
          "details": "1. Identify all tests marked with @pytest.mark.skip\n2. Analyze each skipped test to determine if it's still relevant\n3. Implement missing test logic for relevant tests\n4. Remove tests that are redundant or no longer applicable\n5. Document decisions for each skipped test",
          "status": "done",
          "testStrategy": "Ensure no tests remain with @pytest.mark.skip decorator after completion"
        },
        {
          "id": 4,
          "title": "Complete BDD Scenarios and Replace Pass Statements",
          "description": "Complete partial BDD scenarios (particularly for scoring functionality) and replace any pass statements with actual assertions.",
          "dependencies": [
            2
          ],
          "details": "1. Identify all incomplete BDD scenarios in feature files\n2. Complete step definitions for all scenarios, focusing on scoring functionality\n3. Find all test functions containing pass statements\n4. Replace each pass statement with appropriate assertions\n5. Verify all BDD scenarios run successfully",
          "status": "done",
          "testStrategy": "Run pytest with the bdd plugin to verify all scenarios pass and have complete implementations"
        },
        {
          "id": 5,
          "title": "Implement Test Fixtures and Parameterized Tests",
          "description": "Add test fixtures for common setup/teardown and implement parameterized tests to increase coverage.",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "1. Identify common setup and teardown patterns across tests\n2. Create fixtures in conftest.py for these patterns\n3. Refactor existing tests to use the new fixtures\n4. Identify opportunities for parameterized testing\n5. Implement parameterized tests using pytest.mark.parametrize",
          "status": "done",
          "testStrategy": "Verify reduced code duplication and increased test coverage through pytest-cov reports"
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement Real API Integration Tests",
      "description": "Create optional integration tests for all external API services with environment flag controls and mock fallbacks for CI.",
      "status": "done",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "1. Create integration tests for each external API:\n   - Yelp Fusion\n   - Google Places\n   - ScreenshotOne\n   - OpenAI (GPT-4o)\n   - SendGrid\n2. Implement environment flag controls to enable/disable real API calls:\n   ```python\n   # Example in conftest.py\n   import os\n   import pytest\n   \n   def pytest_addoption(parser):\n       parser.addoption(\"--use-real-apis\", action=\"store_true\", default=False)\n   \n   @pytest.fixture\n   def use_real_apis(request):\n       return request.config.getoption(\"--use-real-apis\")\n   ```\n3. Create mock fallbacks for CI environment:\n   ```python\n   @pytest.fixture\n   def yelp_api(use_real_apis):\n       if use_real_apis and os.getenv(\"YELP_API_KEY\"):\n           return RealYelpAPI()\n       return MockYelpAPI()\n   ```\n4. Add metrics collection for each API call:\n   - Response time\n   - Token usage (for OpenAI)\n   - API cost where applicable\n5. Log metrics to a structured format for analysis\n6. Ensure one real call test per pipeline stage with mock fallback for CI",
      "testStrategy": "1. Verify that tests run with mock APIs by default with `pytest tests/integration`\n2. Confirm that real API tests can be enabled with the appropriate flag using `pytest tests/integration --use-real-apis`\n3. Validate that metrics are correctly captured and logged by checking log files\n4. Test error handling for API failures with intentional error scenarios\n5. Ensure CI uses mock APIs and logs simulated costs by checking CI logs\n6. Create a manual test script to validate real API integration and run it with `python tests/manual_api_validation.py`\n7. Verify that all API keys are properly loaded from environment variables\n8. Run a post-execution healthcheck that confirms all API integration tests exist and pass\n9. Check for the existence of metrics logs after test execution\n10. Verify CI passes with the integration tests in mock mode\n11. Confirm that each API has both mock and real implementations available",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up test environment configuration",
          "description": "Create environment flag controls to toggle between real API calls and mocks during testing",
          "dependencies": [],
          "details": "Implement environment variables or configuration flags that control whether tests use real external APIs or mocks. Create a central configuration module that reads these flags and provides a consistent interface for test files to determine test mode. Include documentation on how to set up local and CI environments.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement API test fixtures and mocks",
          "description": "Create test fixtures and mock implementations for each external API (Yelp, Google Places, ScreenshotOne, OpenAI, SendGrid)",
          "dependencies": [
            1
          ],
          "details": "For each API, create standardized fixtures that represent typical API responses. Implement mock classes or functions that can be used as drop-in replacements for real API clients. Ensure mocks can simulate both successful responses and various error conditions. Store fixtures in a structured directory for easy maintenance.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Develop real API integration tests",
          "description": "Implement actual API integration tests that can run against real external services when enabled",
          "dependencies": [
            1,
            2
          ],
          "details": "Create test suites for each external API that can run against the real services when enabled by environment flags. Include positive and negative test cases. Handle authentication and rate limiting appropriately. Implement proper cleanup to avoid side effects between tests. Add timeout handling for external API calls.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Add metrics collection for API tests",
          "description": "Implement metrics collection for API tests to track performance, success rates, and usage patterns",
          "dependencies": [
            3
          ],
          "details": "Create a metrics collection system that records API call durations, success/failure rates, and usage patterns. Implement logging that captures relevant request/response details while respecting privacy concerns. Add reporting capabilities to visualize test metrics over time. Ensure metrics are collected regardless of whether tests use real or mock APIs.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Configure CI pipeline for API testing",
          "description": "Set up CI configuration to properly run API tests with appropriate controls and fallbacks",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Configure CI pipeline to run API tests with appropriate environment settings. Implement strategies to handle API rate limits and costs in CI environment. Set up secret management for API keys. Configure test reporting and notifications. Ensure tests can fall back to mocks if real API testing fails or is disabled. Add documentation for maintaining and troubleshooting the CI setup.",
          "status": "done"
        }
      ]
    },
    {
      "id": 7,
      "title": "Restructure Module Layout and Utilities",
      "description": "Refactor the codebase structure to use a proper package layout, normalize imports, and centralize configuration loading.",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "1. Create a proper package structure:\n   ```\n   leadfactory/\n     __init__.py\n     pipeline/\n       __init__.py\n       # Move scripts from bin/ here\n     utils/\n       __init__.py\n       # Consolidate from utils/, bin/utils/, metrics.py\n     cost/\n       __init__.py\n       # Move cost_tracker.py, budget_gate.py here\n     config.py  # Centralized configuration\n   ```\n2. Refactor imports to use the new package structure:\n   - Remove all `sys.path.insert()` calls\n   - Use relative imports within packages\n   - Use absolute imports from package root\n3. Update all import statements throughout the codebase\n4. Create proper `__init__.py` files with appropriate exports\n5. Move all configuration loading to `leadfactory/config.py`\n6. Update any scripts or entry points to use the new module structure\n7. Create a `setup.py` or `pyproject.toml` for package installation\n\nExample refactored imports:\n```python\n# Before\nimport sys\nsys.path.insert(0, '../')\nfrom utils.io import read_json\n\n# After\nfrom leadfactory.utils.io import read_json\n# or (if within utils package)\nfrom .io import read_json\n```",
      "testStrategy": "1. Verify that all imports work correctly after refactoring by running `python -c \"import leadfactory\"`\n2. Confirm that CI builds and tests pass with the new structure\n3. Test that the package can be installed and imported with `pip install -e .` followed by `python -c \"import leadfactory\"`\n4. Validate that all path manipulation hacks are removed with `grep -r \"sys.path.insert\" --include=\"*.py\" .`\n5. Ensure that configuration is properly loaded from the central module with a dedicated test\n6. Check that all functionality remains intact after restructuring by running the full test suite\n7. Verify that entry points and scripts still work correctly by executing each one\n8. Run a post-execution healthcheck that confirms the expected directory structure exists\n9. Check for the existence of all required `__init__.py` files with `find leadfactory -type d -exec test -e \"{}/__init__.py\" \\; -print`\n10. Verify that `setup.py` or `pyproject.toml` exists and is valid\n11. Confirm CI passes with the restructured codebase",
      "subtasks": [
        {
          "id": 1,
          "title": "Create new package structure",
          "description": "Design and implement the new directory hierarchy for the module restructuring",
          "dependencies": [],
          "details": "Create the new folder structure according to the design plan. This includes creating main package directories, subpackages, and ensuring proper nesting. Document the new structure in a diagram or text file for reference during the migration process.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Create proper __init__.py files",
          "description": "Add appropriate __init__.py files to all packages and subpackages",
          "dependencies": [
            1
          ],
          "details": "For each package and subpackage in the new structure, create __init__.py files that define what should be exposed at each level. Include proper imports, __all__ declarations, and version information where appropriate. Ensure backward compatibility for important public interfaces.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Centralize configuration loading",
          "description": "Implement a unified configuration system that can be accessed throughout the codebase",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated configuration module that handles loading, validation, and access to configuration settings. Move all existing configuration logic to this central location. Implement a clean API for accessing configuration values from anywhere in the codebase.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Refactor imports throughout codebase",
          "description": "Update all import statements to reflect the new package structure",
          "dependencies": [
            1,
            2
          ],
          "details": "Systematically go through all Python files and update import statements to use the new package paths. Use tools like 'grep' or IDE search functionality to find all import statements. Test each file after updating to ensure imports resolve correctly. Consider creating import compatibility layers if needed.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Update entry points",
          "description": "Modify all application entry points to work with the new structure",
          "dependencies": [
            2,
            4
          ],
          "details": "Identify all entry points (main scripts, CLI commands, etc.) and update them to work with the new package structure. Ensure that command-line arguments and execution paths continue to function as expected. Test each entry point thoroughly after modification.",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Create package installation files",
          "description": "Update or create setup.py, requirements.txt, and other installation-related files",
          "dependencies": [
            1,
            2,
            5
          ],
          "details": "Update the package installation configuration to reflect the new structure. This includes modifying setup.py with the correct package information, updating requirements.txt with dependencies, and ensuring any build scripts or installation documentation is current. Test installation in a clean environment to verify everything works correctly.",
          "status": "done"
        }
      ]
    },
    {
      "id": 8,
      "title": "Enhance Logging, Observability, and Failover Logic",
      "description": "Implement unified logging, verify metrics collection, and validate failover logic for pipeline failures and budget overruns.",
      "status": "done",
      "dependencies": [
        6,
        7
      ],
      "priority": "medium",
      "details": "1. Replace all `print()` statements and `basicConfig()` calls with a unified logger:\n   ```python\n   # In leadfactory/utils/logging.py\n   import logging\n   import sys\n   \n   def setup_logger(name, level=logging.INFO):\n       logger = logging.getLogger(name)\n       logger.setLevel(level)\n       \n       handler = logging.StreamHandler(sys.stdout)\n       formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n       handler.setFormatter(formatter)\n       \n       logger.addHandler(handler)\n       return logger\n   ```\n2. Verify that `PrometheusExporter` correctly exposes metrics:\n   - CPU usage\n   - cost_per_lead\n   - bounce/spam rate\n   - batch completion time\n3. Implement simulation tests for failure scenarios:\n   - Pipeline batch failure → alert flag\n   - Cost overrun → budget gate activation\n4. Unify and test metrics collection and cost tracking\n5. Add structured logging for all pipeline stages\n6. Implement proper error handling with appropriate log levels\n7. Create a dashboard or report template for metrics visualization",
      "testStrategy": "1. Verify that all logging uses the unified logger with `grep -r \"print(\" --include=\"*.py\" . | grep -v \"tests/\"`\n2. Confirm that metrics are correctly exported to Prometheus by checking metric endpoints\n3. Test failure scenarios to ensure alerts and gates are triggered with dedicated tests\n4. Validate that logs contain appropriate context and severity levels by examining log output\n5. Check that CI logs metrics and simulates failure scenarios\n6. Create tests for each metric to ensure accurate collection\n7. Verify that cost tracking is accurate and triggers budget gates with simulation tests\n8. Run a post-execution healthcheck that confirms the logging module exists and is used throughout the codebase\n9. Check for the existence of the metrics exporter and dashboard template\n10. Verify CI passes with all logging and metrics tests\n11. Confirm that failure simulation tests pass and produce expected alerts",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Unified Logging System",
          "description": "Create a centralized logging framework that standardizes log formats and levels across all components of the application.",
          "dependencies": [],
          "details": "Design and implement a unified logging interface that all services can use. Include log levels (DEBUG, INFO, WARN, ERROR), timestamps, service identifiers, and request IDs. Create adapters for existing logging implementations to ensure backward compatibility. Document the new logging standards for the team.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Set Up Metrics Collection and Prometheus Integration",
          "description": "Implement metrics collection throughout the application and integrate with Prometheus for monitoring and alerting.",
          "dependencies": [
            1
          ],
          "details": "Define key metrics to track (latency, throughput, error rates, resource usage). Implement metric collection points in critical code paths. Set up Prometheus exporters for each service. Configure dashboards in Grafana to visualize the metrics. Create alerting rules for critical thresholds.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Implement Structured Logging for All Pipeline Stages",
          "description": "Convert all existing log statements to structured logging format across all pipeline stages.",
          "dependencies": [
            1
          ],
          "details": "Identify all pipeline stages and their current logging implementations. Convert string-based logs to structured JSON format with consistent fields. Add contextual information to logs (pipeline stage, job ID, data size). Ensure logs can be easily parsed and analyzed by log aggregation tools. Update documentation with examples of the new structured log format.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Create Failure Simulation Tests",
          "description": "Develop a suite of tests that simulate various failure scenarios to verify logging and observability under error conditions.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Identify critical failure scenarios (network outages, service crashes, resource exhaustion). Create test fixtures that can trigger these failures in controlled environments. Implement assertions to verify that appropriate logs and metrics are generated during failures. Test alert triggering for critical failures. Document the failure simulation framework for future test development.",
          "status": "done"
        }
      ]
    },
    {
      "id": 9,
      "title": "Audit and Optimize Dependencies",
      "description": "Remove unused dependencies, add missing ones, run security audits, and finalize requirements files for development and production.",
      "status": "done",
      "dependencies": [
        7
      ],
      "priority": "medium",
      "details": "1. Analyze imports to identify unused dependencies:\n   - SQLAlchemy\n   - pytest-mock\n   - Other packages not imported in code\n2. Add missing dependencies:\n   - python-Levenshtein\n   - Any others identified during refactoring\n3. Run security audit tools:\n   ```bash\n   pip-audit\n   # or\n   safety check -r requirements.txt\n   ```\n4. Document any known CVEs in a security report\n5. Finalize separate requirements files:\n   - `requirements.txt` for production dependencies\n   - `requirements-dev.txt` for development dependencies\n6. Add GitHub Dependabot configuration:\n   ```yaml\n   # .github/dependabot.yml\n   version: 2\n   updates:\n     - package-ecosystem: \"pip\"\n       directory: \"/\"\n       schedule:\n         interval: \"weekly\"\n       open-pull-requests-limit: 10\n   ```\n7. Pin all dependencies to exact versions for reproducibility",
      "testStrategy": "1. Verify that all dependencies are actually used in the codebase with a dependency usage analysis\n2. Confirm that all necessary dependencies are included by running the application and tests\n3. Run security audit tools and verify no critical vulnerabilities with `pip-audit -r requirements.txt`\n4. Test installation from both requirements files with `pip install -r requirements.txt` and `pip install -r requirements-dev.txt`\n5. Validate that CI uses the correct requirements file\n6. Check that Dependabot is properly configured by verifying `.github/dependabot.yml` exists\n7. Verify that the application works with the finalized dependencies by running all tests\n8. Run a post-execution healthcheck that confirms all dependencies are properly pinned\n9. Check for the existence of security audit reports\n10. Verify CI passes with the finalized dependencies\n11. Confirm that both requirements files exist and contain all necessary dependencies",
      "subtasks": [
        {
          "id": 1,
          "title": "Analyze and Remove Unused Dependencies",
          "description": "Scan the codebase to identify and remove dependencies that are no longer being used in the project.",
          "dependencies": [],
          "details": "Use dependency analysis tools to identify unused packages. Cross-reference with import statements in the code. Create a list of dependencies to be removed. Update requirements files by removing these dependencies. Test the application to ensure removal doesn't break functionality.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Add Missing Dependencies and Run Security Audits",
          "description": "Identify any dependencies being used but not properly documented, and perform security audits on all dependencies.",
          "dependencies": [
            1
          ],
          "details": "Scan codebase for imports that aren't in requirements files. Add these missing dependencies with appropriate version constraints. Run security audit tools (like safety, npm audit, or dependabot) to identify vulnerabilities. Document any high-risk dependencies that need immediate attention or updates.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Finalize Requirements Files for Different Environments",
          "description": "Create or update separate requirements files for development, testing, and production environments.",
          "dependencies": [
            1,
            2
          ],
          "details": "Organize dependencies into base, development, testing, and production categories. Create separate requirements files for each environment (e.g., requirements.txt, requirements-dev.txt, requirements-test.txt). Ensure development dependencies (like testing tools) aren't included in production. Document the purpose of each requirements file and how to use them. Verify all files with a clean environment installation test.",
          "status": "done"
        }
      ]
    },
    {
      "id": 10,
      "title": "Implement CI Hardening and Final Validation",
      "description": "Promote CI from warning mode to blocking, generate coverage reports, and confirm system behavior for large-scale runs.",
      "status": "in-progress",
      "dependencies": [
        8,
        9
      ],
      "priority": "high",
      "details": "1. Update CI configuration to make all checks blocking:\n   ```yaml\n   # Example GitHub Actions workflow update\n   jobs:\n     test:\n       runs-on: ubuntu-latest\n       steps:\n         # ...\n         - name: Run linting\n           run: |\n             black --check .\n             ruff check .\n             mypy utils/ bin/ tests/\n             bandit -r leadfactory\n         # ...\n   ```\n2. Generate and publish test coverage reports:\n   ```yaml\n   - name: Generate coverage report\n     run: pytest --cov=leadfactory --cov-report=xml\n   \n   - name: Upload coverage to Codecov\n     uses: codecov/codecov-action@v3\n     with:\n       file: ./coverage.xml\n   ```\n3. Implement validation for 10,000-lead full run:\n   - Create a simulation test that verifies cost thresholds\n   - Implement alert testing for failure scenarios\n   - Verify log and metric retention\n4. Add performance benchmarks to CI\n5. Create a final validation checklist\n6. Document CI gates and requirements in README.md",
      "testStrategy": "1. Verify that CI fails on lint errors, type errors, and test failures by introducing temporary errors\n2. Confirm that coverage reports are generated and published by checking Codecov integration\n3. Validate that the system respects cost thresholds in simulation with `python tests/simulation/large_scale_run.py`\n4. Test alert functionality for failure scenarios with dedicated failure simulation tests\n5. Verify log and metric retention for all pipeline stages by examining log files\n6. Run a scaled-down simulation of the full pipeline with `python tests/simulation/pipeline_simulation.py`\n7. Check that all CI gates are properly documented in README.md\n8. Run a post-execution healthcheck that confirms CI configuration is correctly set up\n9. Verify the existence of coverage reports after test runs\n10. Confirm that performance benchmarks are included in CI and produce results\n11. Check that the validation checklist exists and all items pass\n12. Verify CI passes with all hardened checks enabled",
      "subtasks": [
        {
          "id": 1,
          "title": "Audit current CI configuration",
          "description": "Review the existing CI pipeline to identify all checks and their current blocking/non-blocking status.",
          "dependencies": [],
          "details": "Document all existing CI checks, their purpose, current configuration, and impact on the build process. Identify which checks are currently non-blocking and should be converted to blocking status.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Update CI configuration for blocking checks",
          "description": "Modify the CI configuration to make all identified checks blocking for the build process.",
          "dependencies": [
            1
          ],
          "details": "Update CI configuration files to ensure all checks must pass before a build can be considered successful. Implement proper error reporting and notifications for failed checks. Test the changes in a development environment before applying to production.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Implement test coverage reporting",
          "description": "Set up automated test coverage reporting in the CI pipeline.",
          "dependencies": [
            1
          ],
          "details": "Select and integrate an appropriate test coverage tool. Configure the tool to generate reports during CI runs. Set up thresholds for minimum acceptable coverage. Ensure reports are accessible to developers and stored for historical comparison.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Create large-scale validation tests",
          "description": "Develop comprehensive validation tests that can verify system behavior at scale.",
          "dependencies": [
            1
          ],
          "details": "Identify critical system components that require large-scale testing. Design test scenarios that simulate high load or complex data conditions. Implement automated tests that can be run as part of the CI pipeline. Ensure tests are optimized for reasonable execution time.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Integrate validation tests into CI pipeline",
          "description": "Configure the CI pipeline to run the large-scale validation tests at appropriate stages.",
          "dependencies": [
            2,
            4
          ],
          "details": "Determine optimal timing for running large-scale tests (e.g., nightly builds vs. every commit). Update CI configuration to include these tests. Set up appropriate resource allocation for test execution. Configure proper reporting of test results.",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Document CI gates and requirements",
          "description": "Create comprehensive documentation for all CI gates, checks, and requirements.",
          "dependencies": [
            2,
            3,
            5
          ],
          "details": "Document all CI checks and their purpose. Create guidelines for developers on how to address failed checks. Document coverage requirements and how to interpret reports. Create troubleshooting guides for common CI issues. Publish documentation in an accessible location for all team members.",
          "status": "pending"
        }
      ]
    }
  ]
}
