name: Anthrasite Lead-Factory Unified CI

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to run tests against'
        required: true
        default: 'test'
        type: choice
        options:
          - test
          - staging

jobs:
  pre-commit:
    name: Pre-commit Checks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pre-commit ruff black bandit
          if [ -f constraints.txt ]; then
            if [ -f requirements-ci.txt ]; then pip install -r requirements-ci.txt -c constraints.txt; fi
          else
            if [ -f requirements-ci.txt ]; then pip install -r requirements-ci.txt; fi
          fi

      - name: Generate file list for checks
        run: |
          # Create a list of files to check, excluding those specified in pre-commit config
          # This ensures CI checks match local pre-commit behavior
          find . \( -name ".venv" -o -name "venv" \) -prune -o -type f -name "*.py" -print | grep -v "tests/" | grep -v ".cursor/" | grep -v ".github/workflows/" | grep -v "bin/enrich.py" | grep -v "bin/dedupe.py" > files_to_check.txt
          echo "Files to check:"
          cat files_to_check.txt

      - name: Run pre-commit on selected files
        run: |
          # Run pre-commit with specific hooks that are known to work reliably in CI
          pre-commit run ruff --files $(cat files_to_check.txt)
          pre-commit run black --files $(cat files_to_check.txt)
          pre-commit run bandit --files $(cat files_to_check.txt)
          # Run on all files, let config exclude
          pre-commit run --all-files

  lint:
    name: Lint Code
    needs: pre-commit
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort ruff bandit mypy
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: Generate file list for linting
        run: |
          # Generate a cleaner list of files to check, ensuring they exist
          find . \( -name ".venv" -o -name "venv" \) -prune -o -type f -name "*.py" -print | \
            grep -v "tests/" | grep -v ".cursor/" | grep -v ".github/workflows/" | \
            grep -v "bin/enrich.py" | grep -v "bin/dedupe.py" | grep -v ".git/" | grep -v "archive/" > files_to_check.txt

          # Ensure each file exists and is readable
          cat files_to_check.txt | while read file; do
            if [ ! -f "$file" ]; then
              echo "Warning: File does not exist: $file"
              # Remove non-existent files from the list
              grep -v "$file" files_to_check.txt > files_to_check.txt.tmp
              mv files_to_check.txt.tmp files_to_check.txt
            fi
          done

          echo "Files to lint:"
          cat files_to_check.txt

      - name: Lint with ruff, black, bandit, flake8, and mypy
        run: |
          FILES_TO_LINT=$(cat files_to_check.txt | tr '\n' ' ')
          if [ -z "$FILES_TO_LINT" ]; then
            echo "No files to lint based on files_to_check.txt. Skipping linting."
            exit 0
          fi

          # Set up variables to track failures
          FAILURES=0
          FAILURE_DETAILS=""

          echo "Running Ruff on $FILES_TO_LINT..."
          ruff check --ignore F722 $FILES_TO_LINT --fix || {
            echo "::error::Ruff linting found issues. CI will fail."
            FAILURES=$((FAILURES+1))
            FAILURE_DETAILS="$FAILURE_DETAILS\n- Ruff linting errors found"
          }

          echo "Running Black on $FILES_TO_LINT..."
          black --quiet --check $FILES_TO_LINT || {
            echo "::error::Black formatting found issues. CI will fail."
            FAILURES=$((FAILURES+1))
            FAILURE_DETAILS="$FAILURE_DETAILS\n- Black formatting errors found"
          }

          echo "Running Bandit... (runs on ., excludes via config or -x)"
          bandit -r . -x tests,venv,.venv -ll || {
            echo "::error::Bandit security checks found issues. CI will fail."
            FAILURES=$((FAILURES+1))
            FAILURE_DETAILS="$FAILURE_DETAILS\n- Security issues detected by Bandit"
          }

          echo "Running Flake8 on $FILES_TO_LINT..."
          flake8 $FILES_TO_LINT || {
            echo "::error::Flake8 linting found issues. CI will fail."
            FAILURES=$((FAILURES+1))
            FAILURE_DETAILS="$FAILURE_DETAILS\n- Flake8 linting errors found"
          }

          echo "Running Mypy on $FILES_TO_LINT..."
          mypy $FILES_TO_LINT --config-file=mypy.ini || {
            echo "::error::Mypy type checking found issues. CI will fail."
            FAILURES=$((FAILURES+1))
            FAILURE_DETAILS="$FAILURE_DETAILS\n- Type checking errors found by Mypy"
          }

          echo "Linting checks finished. $FAILURES linters reported issues."

          if [ $FAILURES -gt 0 ]; then
            echo "::error::Code quality checks failed:$FAILURE_DETAILS"
            exit 1
          fi

          echo "All code quality checks passed successfully."
          exit 0

  test-core:
    name: Core Unit Tests
    needs: lint
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        ports:
          - 6379:6379
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: leadfactory_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: Setup test environment
        run: |
          # Create necessary directories
          mkdir -p data/html_storage
          mkdir -p data/supabase_usage

          # Setup test database
          python scripts/minimal_db_setup.py --verbose

          # Create .env file with test values
          cp .env.example .env
          sed -i 's/your_api_key_here/test_api_key/g' .env
          sed -i 's/your_sendgrid_api_key_here/test_sendgrid_key/g' .env
          sed -i 's/your_supabase_key_here/test_supabase_key/g' .env

          # Set environment variables for tests
          echo "BOUNCE_RATE_THRESHOLD=0.02" >> .env
          echo "SPAM_RATE_THRESHOLD=0.01" >> .env
          echo "HEALTH_CHECK_FAILURES_THRESHOLD=2" >> .env
          echo "RETENTION_DAYS_DB_BACKUPS=90" >> .env
          echo "MONTHLY_BUDGET=250" >> .env

      - name: Run core tests
        if: always() # Always run, even if previous steps had soft failures
        run: |
          echo "Running core tests..."
          pytest -n auto --dist loadscope --cov=leadfactory --cov-report term-missing --cov-report xml --cov-fail-under=80 tests/
          echo "Core tests finished. Any failures above should be addressed."

      - name: Publish coverage report
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true
          verbose: true

  build-docker:
    name: Build Docker Image
    needs: test-core
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: false
          tags: anthrasite-lead-factory:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

  trigger-large-scale-validation:
    name: Trigger Large-Scale Validation
    needs: [test-core, build-docker]
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.test-core.result == 'success' &&
      needs.build-docker.result == 'success'
    steps:
      - name: Trigger large-scale validation workflow
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'large-scale-validation.yml',
              ref: 'main',
              inputs: {
                skip_10k: 'true',  // Skip 10k test for regular builds to save time
                test_failures: 'true',
                test_bottlenecks: 'true',
                generate_report: 'true'
              }
            });
            console.log('Large-scale validation workflow triggered successfully');

  notify:
    name: Notify on Completion
    needs: [pre-commit, lint, test-core, build-docker]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Check build status
        id: check
        run: |
          if [ "${{ needs.pre-commit.result }}" == "success" ] && [ "${{ needs.lint.result }}" == "success" ] && [ "${{ needs.test-core.result }}" == "success" ] && [ "${{ needs.build-docker.result }}" == "success" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
          fi

      - name: Notify on success
        if: steps.check.outputs.status == 'success'
        run: |
          echo "CI pipeline completed successfully!"
          # Add notification mechanism here (e.g., Slack, email)

      - name: Notify on failure
        if: steps.check.outputs.status == 'failure'
        run: |
          echo "CI pipeline failed!"
          # Add notification mechanism here (e.g., Slack, email)
