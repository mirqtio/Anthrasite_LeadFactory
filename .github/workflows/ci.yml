name: Anthrasite Lead-Factory CI

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to run tests against'
        required: true
        default: 'test'
        type: choice
        options:
          - test
          - staging

jobs:
  lint:
    name: Lint Code
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort pre-commit
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: Run pre-commit hooks
        run: |
          # Create a temporary pre-commit config that matches our local setup
          # This ensures CI matches local pre-commit behavior
          cat > .pre-commit-config-ci.yaml << 'EOL'
          repos:
          -   repo: https://github.com/astral-sh/ruff-pre-commit
              rev: v0.9.5
              hooks:
                - id: ruff
                  args: [--fix, --exit-non-zero-on-fix]
                  types: [python]
                  exclude: "^(venv|.venv|env|.env|.git|.github|.mypy_cache|.pytest_cache|__pycache__|build|dist|node_modules|tasks|.ruff_cache)"

          -   repo: https://github.com/psf/black-pre-commit-mirror
              rev: 24.4.2
              hooks:
                - id: black
                  language_version: python3.10
                  types: [python]
                  exclude: "^(venv|.venv|env|.env|.git|.github|.mypy_cache|.pytest_cache|__pycache__|build|dist|node_modules|tasks|.ruff_cache|scripts/ci_format.py|bin/enrich.py|bin/dedupe.py)"
                  args: [--check, --config, .black.toml, --diff]

          -   repo: https://github.com/PyCQA/isort
              rev: 5.12.0
              hooks:
                - id: isort
                  name: isort (python)
                  args: [--check, --diff]
                  types: [python]
                  exclude: "^(venv|.venv|env|.env|.git|.github|.mypy_cache|.pytest_cache|__pycache__|build|dist|node_modules|tasks|.ruff_cache|scripts/ci_format.py|bin/enrich.py|bin/dedupe.py)"

          -   repo: https://github.com/PyCQA/bandit
              rev: 1.7.5
              hooks:
                - id: bandit
                  args: ['-ll', '-i']
                  types: [python]
                  exclude: "^(venv|.venv|env|.env|.git|.github|.mypy_cache|.pytest_cache|__pycache__|build|dist|node_modules|tasks|tests|.ruff_cache)"

          -   repo: https://github.com/pre-commit/pre-commit-hooks
              rev: v4.4.0
              hooks:
                - id: trailing-whitespace
                - id: end-of-file-fixer
                - id: check-yaml
                - id: check-added-large-files
                - id: debug-statements
                - id: check-ast
          EOL

          # Run pre-commit with our CI-specific config
          SKIP=flake8,run-ci-format-script pre-commit run --all-files --config .pre-commit-config-ci.yaml

      - name: Lint with flake8
        run: |
          # Skip files that are excluded in the pre-commit config
          find . -name "*.py" | grep -v "venv\|.venv\|env\|.env\|.git\|.github\|.mypy_cache\|.pytest_cache\|__pycache__\|build\|dist\|node_modules\|tasks\|.ruff_cache\|scripts/ci_format.py\|bin/enrich.py\|bin/dedupe.py" > files_to_check.txt
          # stop the build if there are Python syntax errors or undefined names
          if [ -s files_to_check.txt ]; then
            cat files_to_check.txt | xargs flake8 --count --select=E9,F63,F7,F82 --show-source --statistics
            # exit-zero treats all errors as warnings
            cat files_to_check.txt | xargs flake8 --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
          else
            echo "No files to check with flake8"
          fi

      - name: Check formatting with black
        run: |
          # Use the same files list from flake8 step
          if [ -s files_to_check.txt ]; then
            cat files_to_check.txt | xargs black --check --config .black.toml --diff
          else
            echo "No files to check with black"
          fi

      - name: Check imports with isort
        run: |
          # Use the same files list from flake8 step
          if [ -s files_to_check.txt ]; then
            cat files_to_check.txt | xargs isort --check --diff
          else
            echo "No files to check with isort"
          fi

  test:
    name: Run Tests
    needs: lint
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        ports:
          - 6379:6379
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: leadfactory_test
        ports:
          - 5432:5432
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    strategy:
      matrix:
        python-version: ['3.10']

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-bdd
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: Create .env file
        run: |
          cp .env.example .env
          # Set mock API keys for testing
          sed -i 's/your_yelp_api_key_here/mock_yelp_key/g' .env
          sed -i 's/your_google_places_api_key_here/mock_google_key/g' .env
          sed -i 's/your_openai_api_key_here/mock_openai_key/g' .env
          sed -i 's/your_anthropic_api_key_here/mock_anthropic_key/g' .env
          sed -i 's/your_sendgrid_api_key_here/mock_sendgrid_key/g' .env
          # Set database URL for testing
          sed -i 's|postgresql://postgres:postgres@localhost:5432/leadfactory|postgresql://postgres:postgres@localhost:5432/leadfactory_test|g' .env

      - name: Run tests
        env:
          # Email deliverability thresholds
          BOUNCE_RATE_THRESHOLD: 0.02
          SPAM_RATE_THRESHOLD: 0.001
          MONTHLY_BUDGET: 250
          # SendGrid configuration for testing
          SENDGRID_IP_POOL_NAMES: "primary,secondary,tertiary"
          SENDGRID_SUBUSER_NAMES: "primary,secondary,tertiary"
        run: |
          pytest --cov=. --cov-report=xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: false

      - name: Run BDD acceptance tests
        run: |
          pytest tests/ --bdd -v

  validate-db:
    name: Validate Database Schema
    needs: test
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: leadfactory_test
        ports:
          - 5432:5432
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install sqlite3-utils psycopg2-binary

      - name: Validate SQL migration scripts
        run: |
          # Create a temporary SQLite database
          sqlite3 test.db < db/migrations/2025-05-19_init.sql

          # Verify SQLite database structure
          sqlite3-utils tables test.db --json

          # Check for required tables in SQLite
          python -c "
          import sqlite3, sys
          required_tables = ['businesses', 'zip_queue', 'verticals', 'features', 'mockups', 'emails', 'cost_tracking', 'email_metrics']
          conn = sqlite3.connect('test.db')
          cursor = conn.cursor()
          cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")
          tables = [row[0] for row in cursor.fetchall()]
          missing = [table for table in required_tables if table not in tables]
          if missing:
              print(f'Missing required tables in SQLite: {missing}')
              sys.exit(1)
          else:
              print('All required tables exist in SQLite')
          conn.close()
          "

      - name: Create migration script for Postgres
        run: |
          # Create a migration script for Postgres
          cat > scripts/postgres_schema.sql << 'EOL'
          -- Businesses table
          CREATE TABLE IF NOT EXISTS businesses (
              id SERIAL PRIMARY KEY,
              name TEXT NOT NULL,
              address TEXT,
              city TEXT,
              state TEXT,
              zip TEXT,
              phone TEXT,
              email TEXT,
              website TEXT,
              category TEXT,
              source TEXT,
              source_id TEXT,
              status TEXT DEFAULT 'pending',
              score INTEGER DEFAULT 0,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Features table
          CREATE TABLE IF NOT EXISTS features (
              id SERIAL PRIMARY KEY,
              business_id INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              tech_stack JSONB,
              social_profiles JSONB,
              business_type TEXT,
              employee_count INTEGER,
              annual_revenue NUMERIC,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Mockups table
          CREATE TABLE IF NOT EXISTS mockups (
              id SERIAL PRIMARY KEY,
              business_id INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              image_url TEXT,
              html_content TEXT,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Emails table
          CREATE TABLE IF NOT EXISTS emails (
              id SERIAL PRIMARY KEY,
              business_id INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              subject TEXT,
              body_html TEXT,
              body_text TEXT,
              status TEXT DEFAULT 'pending',
              sent_at TIMESTAMP,
              message_id TEXT,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Candidate duplicate pairs table
          CREATE TABLE IF NOT EXISTS candidate_duplicate_pairs (
              id SERIAL PRIMARY KEY,
              business_id_1 INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              business_id_2 INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              similarity_score NUMERIC,
              status TEXT DEFAULT 'pending',
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              CONSTRAINT unique_pair UNIQUE (business_id_1, business_id_2)
          );

          -- Business merges table
          CREATE TABLE IF NOT EXISTS business_merges (
              id SERIAL PRIMARY KEY,
              source_business_id INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              target_business_id INTEGER REFERENCES businesses(id) ON DELETE CASCADE,
              merged_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Zip queue table
          CREATE TABLE IF NOT EXISTS zip_queue (
              id SERIAL PRIMARY KEY,
              zip TEXT NOT NULL UNIQUE,
              priority INTEGER DEFAULT 0,
              done BOOLEAN DEFAULT FALSE,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Verticals table
          CREATE TABLE IF NOT EXISTS verticals (
              id SERIAL PRIMARY KEY,
              name TEXT NOT NULL UNIQUE,
              priority INTEGER DEFAULT 0,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Cost tracking table
          CREATE TABLE IF NOT EXISTS cost_tracking (
              id SERIAL PRIMARY KEY,
              service TEXT NOT NULL,
              cost NUMERIC NOT NULL,
              date DATE NOT NULL,
              details JSONB,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Email metrics table
          CREATE TABLE IF NOT EXISTS email_metrics (
              id SERIAL PRIMARY KEY,
              date DATE NOT NULL,
              sent INTEGER DEFAULT 0,
              delivered INTEGER DEFAULT 0,
              opened INTEGER DEFAULT 0,
              clicked INTEGER DEFAULT 0,
              bounced INTEGER DEFAULT 0,
              spam_reports INTEGER DEFAULT 0,
              unsubscribes INTEGER DEFAULT 0,
              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );

          -- Create indexes
          CREATE INDEX IF NOT EXISTS idx_businesses_name ON businesses(name);
          CREATE INDEX IF NOT EXISTS idx_businesses_zip ON businesses(zip);
          CREATE INDEX IF NOT EXISTS idx_businesses_status ON businesses(status);
          CREATE INDEX IF NOT EXISTS idx_businesses_score ON businesses(score);
          CREATE INDEX IF NOT EXISTS idx_features_business_id ON features(business_id);
          CREATE INDEX IF NOT EXISTS idx_mockups_business_id ON mockups(business_id);
          CREATE INDEX IF NOT EXISTS idx_emails_business_id ON emails(business_id);
          CREATE INDEX IF NOT EXISTS idx_emails_status ON emails(status);
          CREATE INDEX IF NOT EXISTS idx_candidate_duplicate_pairs_status ON candidate_duplicate_pairs(status);
          CREATE INDEX IF NOT EXISTS idx_zip_queue_done ON zip_queue(done);
          CREATE INDEX IF NOT EXISTS idx_cost_tracking_service ON cost_tracking(service);
          CREATE INDEX IF NOT EXISTS idx_cost_tracking_date ON cost_tracking(date);
          CREATE INDEX IF NOT EXISTS idx_email_metrics_date ON email_metrics(date);
          EOL

      - name: Validate Postgres schema
        run: |
          # Apply schema to Postgres
          PGPASSWORD=postgres psql -h localhost -U postgres -d leadfactory_test -f scripts/postgres_schema.sql

          # Check for required tables in Postgres
          python -c "
          import psycopg2, sys
          required_tables = ['businesses', 'zip_queue', 'verticals', 'features', 'mockups', 'emails', 'cost_tracking', 'email_metrics']
          conn = psycopg2.connect('postgresql://postgres:postgres@localhost:5432/leadfactory_test')
          cursor = conn.cursor()
          cursor.execute(\"SELECT tablename FROM pg_tables WHERE schemaname = 'public'\")
          tables = [row[0] for row in cursor.fetchall()]
          missing = [table for table in required_tables if table not in tables]
          if missing:
              print(f'Missing required tables in Postgres: {missing}')
              sys.exit(1)
          else:
              print('All required tables exist in Postgres')
          conn.close()
          "

  build-docker:
    name: Build Docker Image
    needs: [test, validate-db]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: false
          tags: anthrasite/lead-factory:test
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/lead-factory.tar

      - name: Upload Docker image artifact
        uses: actions/upload-artifact@v3
        with:
          name: lead-factory-docker
          path: /tmp/lead-factory.tar
          retention-days: 1

  notify:
    name: Notify on Completion
    needs: [build-docker]
    if: always()
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        ports:
          - 6379:6379
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: leadfactory_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Check build status
        id: status
        run: |
          if [ "${{ needs.lint.result }}" == "success" ] && [ "${{ needs.test.result }}" == "success" ] && [ "${{ needs.validate-db.result }}" == "success" ] && [ "${{ needs.build-docker.result }}" == "success" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
          fi

      - name: Notify on success
        if: steps.status.outputs.status == 'success'
        run: |
          echo "CI pipeline completed successfully!"
          # Add notification mechanism here (e.g., Slack, email)

      - name: Notify on failure
        if: steps.status.outputs.status == 'failure'
        run: |
          echo "CI pipeline failed!"
          # Add notification mechanism here (e.g., Slack, email)
